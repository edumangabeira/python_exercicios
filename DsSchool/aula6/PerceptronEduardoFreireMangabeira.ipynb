{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "equivalent-mills",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: sklearn in c:\\users\\eduardo\\miniconda3\\lib\\site-packages (0.0)\n",
      "Requirement already satisfied, skipping upgrade: scikit-learn in c:\\users\\eduardo\\miniconda3\\lib\\site-packages (from sklearn) (0.24.1)\n",
      "Requirement already satisfied, skipping upgrade: scipy>=0.19.1 in c:\\users\\eduardo\\miniconda3\\lib\\site-packages (from scikit-learn->sklearn) (1.6.2)\n",
      "Requirement already satisfied, skipping upgrade: threadpoolctl>=2.0.0 in c:\\users\\eduardo\\miniconda3\\lib\\site-packages (from scikit-learn->sklearn) (2.1.0)\n",
      "Requirement already satisfied, skipping upgrade: joblib>=0.11 in c:\\users\\eduardo\\miniconda3\\lib\\site-packages (from scikit-learn->sklearn) (1.0.1)\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.13.3 in c:\\users\\eduardo\\miniconda3\\lib\\site-packages (from scikit-learn->sklearn) (1.20.2)\n",
      "Requirement already up-to-date: pandas in c:\\users\\eduardo\\miniconda3\\lib\\site-packages (1.2.3)\n",
      "Requirement already satisfied, skipping upgrade: python-dateutil>=2.7.3 in c:\\users\\eduardo\\miniconda3\\lib\\site-packages (from pandas) (2.8.1)\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.16.5 in c:\\users\\eduardo\\miniconda3\\lib\\site-packages (from pandas) (1.20.2)\n",
      "Requirement already satisfied, skipping upgrade: pytz>=2017.3 in c:\\users\\eduardo\\miniconda3\\lib\\site-packages (from pandas) (2021.1)\n",
      "Requirement already satisfied, skipping upgrade: six>=1.5 in c:\\users\\eduardo\\miniconda3\\lib\\site-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n",
      "Requirement already up-to-date: matplotlib in c:\\users\\eduardo\\miniconda3\\lib\\site-packages (3.4.0)\n",
      "Requirement already satisfied, skipping upgrade: python-dateutil>=2.7 in c:\\users\\eduardo\\miniconda3\\lib\\site-packages (from matplotlib) (2.8.1)\n",
      "Requirement already satisfied, skipping upgrade: cycler>=0.10 in c:\\users\\eduardo\\miniconda3\\lib\\site-packages (from matplotlib) (0.10.0)\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.16 in c:\\users\\eduardo\\miniconda3\\lib\\site-packages (from matplotlib) (1.20.2)\n",
      "Requirement already satisfied, skipping upgrade: pillow>=6.2.0 in c:\\users\\eduardo\\miniconda3\\lib\\site-packages (from matplotlib) (8.1.2)\n",
      "Requirement already satisfied, skipping upgrade: pyparsing>=2.2.1 in c:\\users\\eduardo\\miniconda3\\lib\\site-packages (from matplotlib) (2.4.7)\n",
      "Requirement already satisfied, skipping upgrade: kiwisolver>=1.0.1 in c:\\users\\eduardo\\miniconda3\\lib\\site-packages (from matplotlib) (1.3.1)\n",
      "Requirement already satisfied, skipping upgrade: six>=1.5 in c:\\users\\eduardo\\miniconda3\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.15.0)\n",
      "Requirement already up-to-date: keras in c:\\users\\eduardo\\miniconda3\\lib\\site-packages (2.4.3)\n",
      "Requirement already satisfied, skipping upgrade: scipy>=0.14 in c:\\users\\eduardo\\miniconda3\\lib\\site-packages (from keras) (1.6.2)\n",
      "Requirement already satisfied, skipping upgrade: pyyaml in c:\\users\\eduardo\\miniconda3\\lib\\site-packages (from keras) (5.4.1)\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.9.1 in c:\\users\\eduardo\\miniconda3\\lib\\site-packages (from keras) (1.20.2)\n",
      "Requirement already satisfied, skipping upgrade: h5py in c:\\users\\eduardo\\miniconda3\\lib\\site-packages (from keras) (3.2.1)\n",
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.4.1-cp38-cp38-win_amd64.whl (370.7 MB)\n",
      "Collecting typing-extensions~=3.7.4\n",
      "  Downloading typing_extensions-3.7.4.3-py3-none-any.whl (22 kB)\n",
      "Collecting opt-einsum~=3.3.0\n",
      "  Downloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "Collecting grpcio~=1.32.0\n",
      "  Downloading grpcio-1.32.0-cp38-cp38-win_amd64.whl (2.6 MB)\n",
      "Collecting h5py~=2.10.0\n",
      "  Downloading h5py-2.10.0-cp38-cp38-win_amd64.whl (2.5 MB)\n",
      "Collecting flatbuffers~=1.12.0\n",
      "  Downloading flatbuffers-1.12-py2.py3-none-any.whl (15 kB)\n",
      "Collecting tensorflow-estimator<2.5.0,>=2.4.0\n",
      "  Downloading tensorflow_estimator-2.4.0-py2.py3-none-any.whl (462 kB)\n",
      "Collecting astunparse~=1.6.3\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Requirement already satisfied, skipping upgrade: six~=1.15.0 in c:\\users\\eduardo\\miniconda3\\lib\\site-packages (from tensorflow) (1.15.0)\n",
      "Collecting absl-py~=0.10\n",
      "  Downloading absl_py-0.12.0-py3-none-any.whl (129 kB)\n",
      "Collecting wrapt~=1.12.1\n",
      "  Downloading wrapt-1.12.1.tar.gz (27 kB)\n",
      "Collecting gast==0.3.3\n",
      "  Downloading gast-0.3.3-py2.py3-none-any.whl (9.7 kB)\n",
      "Collecting google-pasta~=0.2\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Collecting tensorboard~=2.4\n",
      "  Downloading tensorboard-2.4.1-py3-none-any.whl (10.6 MB)\n",
      "Collecting keras-preprocessing~=1.1.2\n",
      "  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
      "Requirement already satisfied, skipping upgrade: wheel~=0.35 in c:\\users\\eduardo\\miniconda3\\lib\\site-packages (from tensorflow) (0.35.1)\n",
      "Collecting termcolor~=1.1.0\n",
      "  Downloading termcolor-1.1.0.tar.gz (3.9 kB)\n",
      "Collecting numpy~=1.19.2\n",
      "  Downloading numpy-1.19.5-cp38-cp38-win_amd64.whl (13.3 MB)\n",
      "Collecting protobuf>=3.9.2\n",
      "  Downloading protobuf-3.15.6-py2.py3-none-any.whl (173 kB)\n",
      "Collecting google-auth<2,>=1.6.3\n",
      "  Downloading google_auth-1.28.0-py2.py3-none-any.whl (136 kB)\n",
      "Requirement already satisfied, skipping upgrade: setuptools>=41.0.0 in c:\\users\\eduardo\\miniconda3\\lib\\site-packages (from tensorboard~=2.4->tensorflow) (50.3.1.post20201107)\n",
      "Collecting markdown>=2.6.8\n",
      "  Downloading Markdown-3.3.4-py3-none-any.whl (97 kB)\n",
      "Collecting google-auth-oauthlib<0.5,>=0.4.1\n",
      "  Downloading google_auth_oauthlib-0.4.3-py2.py3-none-any.whl (18 kB)\n",
      "Collecting tensorboard-plugin-wit>=1.6.0\n",
      "  Downloading tensorboard_plugin_wit-1.8.0-py3-none-any.whl (781 kB)\n",
      "Requirement already satisfied, skipping upgrade: requests<3,>=2.21.0 in c:\\users\\eduardo\\miniconda3\\lib\\site-packages (from tensorboard~=2.4->tensorflow) (2.24.0)\n",
      "Collecting werkzeug>=0.11.15\n",
      "  Downloading Werkzeug-1.0.1-py2.py3-none-any.whl (298 kB)\n",
      "Collecting pyasn1-modules>=0.2.1\n",
      "  Downloading pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n",
      "Collecting cachetools<5.0,>=2.0.0"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not install packages due to an EnvironmentError: [WinError 5] Acesso negado: 'C:\\\\Users\\\\Eduardo\\\\miniconda3\\\\Lib\\\\site-packages\\\\~umpy\\\\.libs\\\\libopenblas.GK7GX5KEQ4F6UYO3P26ULGBQYHGQO7J4.gfortran-win_amd64.dll'\n",
      "Consider using the `--user` option or check the permissions.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Downloading cachetools-4.2.1-py3-none-any.whl (12 kB)\n",
      "Collecting rsa<5,>=3.1.4; python_version >= \"3.6\"\n",
      "  Downloading rsa-4.7.2-py3-none-any.whl (34 kB)\n",
      "Collecting requests-oauthlib>=0.7.0\n",
      "  Downloading requests_oauthlib-1.3.0-py2.py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in c:\\users\\eduardo\\miniconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow) (2020.12.5)\n",
      "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in c:\\users\\eduardo\\miniconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow) (2.10)\n",
      "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\eduardo\\miniconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow) (1.25.11)\n",
      "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in c:\\users\\eduardo\\miniconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow) (3.0.4)\n",
      "Collecting pyasn1<0.5.0,>=0.4.6\n",
      "  Downloading pyasn1-0.4.8-py2.py3-none-any.whl (77 kB)\n",
      "Collecting oauthlib>=3.0.0\n",
      "  Downloading oauthlib-3.1.0-py2.py3-none-any.whl (147 kB)\n",
      "Building wheels for collected packages: wrapt, termcolor\n",
      "  Building wheel for wrapt (setup.py): started\n",
      "  Building wheel for wrapt (setup.py): finished with status 'done'\n",
      "  Created wheel for wrapt: filename=wrapt-1.12.1-py3-none-any.whl size=19558 sha256=2f0d38c2c5600117474c961cc12df15875bbcaf4d7e851553fc63d277ac0c471\n",
      "  Stored in directory: c:\\users\\eduardo\\appdata\\local\\pip\\cache\\wheels\\5f\\fd\\9e\\b6cf5890494cb8ef0b5eaff72e5d55a70fb56316007d6dfe73\n",
      "  Building wheel for termcolor (setup.py): started\n",
      "  Building wheel for termcolor (setup.py): finished with status 'done'\n",
      "  Created wheel for termcolor: filename=termcolor-1.1.0-py3-none-any.whl size=4835 sha256=9fd1248fbf4700a5176b70acf83dc54ace7254c6497ea3d5edb23d401f7ec887\n",
      "  Stored in directory: c:\\users\\eduardo\\appdata\\local\\pip\\cache\\wheels\\a0\\16\\9c\\5473df82468f958445479c59e784896fa24f4a5fc024b0f501\n",
      "Successfully built wrapt termcolor\n",
      "Installing collected packages: typing-extensions, numpy, opt-einsum, grpcio, h5py, flatbuffers, tensorflow-estimator, astunparse, absl-py, wrapt, gast, google-pasta, pyasn1, pyasn1-modules, cachetools, rsa, google-auth, markdown, protobuf, oauthlib, requests-oauthlib, google-auth-oauthlib, tensorboard-plugin-wit, werkzeug, tensorboard, keras-preprocessing, termcolor, tensorflow\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.20.2\n",
      "    Uninstalling numpy-1.20.2:\n",
      "      Successfully uninstalled numpy-1.20.2\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade sklearn\n",
    "!pip install --upgrade pandas\n",
    "!pip install --upgrade matplotlib\n",
    "!pip install --upgrade keras\n",
    "!pip install --upgrade tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "expensive-disposition",
   "metadata": {},
   "source": [
    "## 1 - Implementando o Perceptron com o sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "interracial-treasury",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1])"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Retirado do livro \"Mãos à obra: aprendizado de máquina com ScikitLearn & TensorFlow\" - Aurélien Géron.\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.linear_model import Perceptron\n",
    "\n",
    "iris = load_iris()\n",
    "x = iris.data[:, (2,3)] # comprimento da pétala, largura da pétala\n",
    "y = (iris.target == 0).astype(int) # Iris setosa?\n",
    "\n",
    "per_clf = Perceptron(random_state=42)\n",
    "per_clf.fit(x, y)\n",
    "\n",
    "# pétala na posição [1, 0.5]\n",
    "y_pred = per_clf.predict([[1, 0.5]])\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "treated-championship",
   "metadata": {},
   "source": [
    "###### O resultado \"[1]\" indica que a pétala escolhida pertence ao grupo Iris setosa, caso o resultado fosse \"[0]\", a pétala não pertenceria a esse grupo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "advanced-seating",
   "metadata": {},
   "source": [
    "## Construindo a classe Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "returning-prefix",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyPerceptron:\n",
    "    ''' \n",
    "    Feito a partir de: https://github.com/python-engineer/MLfromscratch/\n",
    "    e a partir do livro \"Mãos à obra: aprendizado de máquina com ScikitLearn & TensorFlow\" - Aurélien Géron.\n",
    "\n",
    "    Atributos:\n",
    "    learning_rate - taxa de aprendizado da rede neural.\n",
    "    n_iters - número de iterações.\n",
    "    heaviside - função de ativação.\n",
    "    weights - pesos.\n",
    "    bias - viés associado.\n",
    "\n",
    "    Métodos:\n",
    "    fit(x, y) - Ajusta modelo linear com gradiente descendente.\n",
    "    predict(x) - Prediz rótulos de classe para amostras em x.\n",
    "    heaviside(x) - implementa a função de ativação.\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, learning_rate = 0.01, n_iters = 1000):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.n_iters = n_iters\n",
    "        self.heaviside = self.heaviside\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "\n",
    "    def fit(self, x, y):\n",
    "        samples, features = x.shape\n",
    "        self.weights = np.zeros(features)\n",
    "        self.bias = 0\n",
    "\n",
    "        y_ = np.array([1 if i > 0 else 0 for i in y])\n",
    "\n",
    "        for _ in range(self.n_iters):\n",
    "            \n",
    "            for idx, x_i in enumerate(x):\n",
    "\n",
    "                linear_output = np.dot(x_i, self.weights) + self.bias\n",
    "                y_predicted = self.heaviside(linear_output)\n",
    "                \n",
    "                # regra de atualização do perceptron\n",
    "                update = self.learning_rate * (y_[idx] - y_predicted)\n",
    "\n",
    "                self.weights += update * x_i\n",
    "                self.bias += update\n",
    "\n",
    "    def predict(self, x):\n",
    "        linear_output = np.dot(x, self.weights) + self.bias\n",
    "        y_predicted = self.heaviside(linear_output)\n",
    "        return y_predicted\n",
    "\n",
    "    def heaviside(self, x):\n",
    "        return np.where(x>=0, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "breathing-muslim",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "    Feito a partir de: https://github.com/python-engineer/MLfromscratch/\n",
      "    e a partir do livro \"Mãos à obra: aprendizado de máquina com ScikitLearn & TensorFlow\" - Aurélien Géron.\n",
      "\n",
      "    Atributos:\n",
      "    learning_rate - taxa de aprendizado da rede neural.\n",
      "    n_iters - número de iterações.\n",
      "    heaviside - função de ativação.\n",
      "    weights - pesos.\n",
      "    bias - viés associado.\n",
      "\n",
      "    Métodos:\n",
      "    fit(x, y) - Ajusta modelo linear com gradiente descendente.\n",
      "    predict(x) - Prediz rótulos de classe para amostras em x.\n",
      "    heaviside(x) - implementa a função de ativação.\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "print(MyPerceptron.__doc__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "institutional-pound",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = iris.data[:, (2,3)]\n",
    "y = (iris.target == 0).astype(int)\n",
    "\n",
    "perceptron = MyPerceptron(learning_rate=0.01, n_iters=1000)\n",
    "perceptron.fit(x, y)\n",
    "predictions = perceptron.predict([[1, 0.5]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "material-updating",
   "metadata": {},
   "source": [
    "###### O resultado \"[1]\" é o mesmo obtido pela função do ScikitLearn, a pétala escolhida é Iris setosa."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fancy-dublin",
   "metadata": {},
   "source": [
    "### Agora vamos reutilizar o modelo estimando a incerteza associada:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "czech-package",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácia média:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>média</th>\n",
       "      <th>Desvio padrão</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>scores</th>\n",
       "      <td>0.995556</td>\n",
       "      <td>0.01663</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           média  Desvio padrão\n",
       "scores  0.995556        0.01663"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "import pandas as pd\n",
    "\n",
    "x = iris.data[:, (2,3)]\n",
    "y = (iris.target == 0).astype(int)\n",
    "model = Perceptron()\n",
    "\n",
    "cross_validation = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=42)\n",
    "scores = cross_val_score(model, x, y, scoring='accuracy', cv=cross_validation, n_jobs=-1)\n",
    "print('Acurácia média:')\n",
    "pd.DataFrame(data = {'média': [np.mean(scores)], 'Desvio padrão': [np.std(scores)]}, index = ['scores'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "generic-moses",
   "metadata": {},
   "source": [
    "Achei o resultado acima um pouco esquisito, seria um caso de overfitting? Eu não dividi os grupos em treino e teste porque em tese ```RepeatedStratifiedKFold``` já faz esse papel. Não entendi como interpretar esses valores."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "preceding-simple",
   "metadata": {},
   "source": [
    "## 2 - Comparando regressão linear com redes neurais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "minute-wichita",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def generate_data(seed, plot=False):\n",
    "    x = np.array([i*np.pi/180 for i in range(30, 400, 4)])\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    y = np.sin(x) + 0.5*np.sin(3*x) + np.random.normal(0,.4,x.shape[0])\n",
    "    data = pd.DataFrame(np.column_stack([x,y]), columns = ['x','y'])\n",
    "    if plot:\n",
    "        plt.plot(data['x'], data['y'], '.')\n",
    "    return data\n",
    "data = generate_data(42, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "corresponding-dining",
   "metadata": {},
   "source": [
    "### Regressão linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "regular-round",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style  type=\"text/css\" >\n",
       "</style><table id=\"T_e7636_\" ><thead>    <tr>        <th class=\"col_heading level0 col0\" >Coef. angular</th>        <th class=\"col_heading level0 col1\" >Intercepto</th>        <th class=\"col_heading level0 col2\" >EQM</th>    </tr></thead><tbody>\n",
       "                <tr>\n",
       "                                <td id=\"T_e7636_row0_col0\" class=\"data row0 col0\" >-0.167884</td>\n",
       "                        <td id=\"T_e7636_row0_col1\" class=\"data row0 col1\" >0.642177</td>\n",
       "                        <td id=\"T_e7636_row0_col2\" class=\"data row0 col2\" >0.507869</td>\n",
       "            </tr>\n",
       "    </tbody></table>"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x17e4e3bdbb0>"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(data['x'].values.reshape((-1, 1)),data['y'], test_size=0.33, random_state=42)\n",
    "reg = LinearRegression().fit(x_train, y_train)\n",
    "y_pred = reg.predict(x_test)\n",
    "# erro quadrático médio\n",
    "eqm = metrics.mean_squared_error(y_test, y_pred)\n",
    "df = pd.DataFrame(data = {'Coef. angular': [reg.coef_[0]], 'Intercepto': [reg.intercept_], 'EQM': [eqm]})\n",
    "df.style.hide_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stylish-valve",
   "metadata": {},
   "source": [
    "### Rede neural\n",
    "\n",
    "Aqui já não usei o Perceptron, pois compreendi que o propósito dele é de classificação. Também não posso mais usar acurácia como métrica, pois é usada para problemas de classificação."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "organizational-header",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.8520 - mean_squared_error: 0.8520\n",
      "Epoch 2/100\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.6576 - mean_squared_error: 0.6576\n",
      "Epoch 3/100\n",
      "7/7 [==============================] - 0s 833us/step - loss: 0.6890 - mean_squared_error: 0.6890\n",
      "Epoch 4/100\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.6805 - mean_squared_error: 0.6805\n",
      "Epoch 5/100\n",
      "7/7 [==============================] - 0s 999us/step - loss: 0.5470 - mean_squared_error: 0.5470\n",
      "Epoch 6/100\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.4466 - mean_squared_error: 0.4466\n",
      "Epoch 7/100\n",
      "7/7 [==============================] - 0s 999us/step - loss: 0.4297 - mean_squared_error: 0.4297\n",
      "Epoch 8/100\n",
      "7/7 [==============================] - 0s 833us/step - loss: 0.3875 - mean_squared_error: 0.3875\n",
      "Epoch 9/100\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.3907 - mean_squared_error: 0.3907\n",
      "Epoch 10/100\n",
      "7/7 [==============================] - 0s 833us/step - loss: 0.3152 - mean_squared_error: 0.3152\n",
      "Epoch 11/100\n",
      "7/7 [==============================] - 0s 999us/step - loss: 0.3383 - mean_squared_error: 0.3383\n",
      "Epoch 12/100\n",
      "7/7 [==============================] - 0s 833us/step - loss: 0.4010 - mean_squared_error: 0.4010\n",
      "Epoch 13/100\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.3057 - mean_squared_error: 0.3057\n",
      "Epoch 14/100\n",
      "7/7 [==============================] - 0s 833us/step - loss: 0.2908 - mean_squared_error: 0.2908\n",
      "Epoch 15/100\n",
      "7/7 [==============================] - 0s 999us/step - loss: 0.2608 - mean_squared_error: 0.2608\n",
      "Epoch 16/100\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.2545 - mean_squared_error: 0.2545\n",
      "Epoch 17/100\n",
      "7/7 [==============================] - 0s 999us/step - loss: 0.3323 - mean_squared_error: 0.3323\n",
      "Epoch 18/100\n",
      "7/7 [==============================] - 0s 999us/step - loss: 0.2851 - mean_squared_error: 0.2851\n",
      "Epoch 19/100\n",
      "7/7 [==============================] - 0s 833us/step - loss: 0.2886 - mean_squared_error: 0.2886\n",
      "Epoch 20/100\n",
      "7/7 [==============================] - 0s 999us/step - loss: 0.3598 - mean_squared_error: 0.3598\n",
      "Epoch 21/100\n",
      "7/7 [==============================] - 0s 833us/step - loss: 0.2916 - mean_squared_error: 0.2916\n",
      "Epoch 22/100\n",
      "7/7 [==============================] - 0s 999us/step - loss: 0.3448 - mean_squared_error: 0.3448\n",
      "Epoch 23/100\n",
      "7/7 [==============================] - 0s 833us/step - loss: 0.2690 - mean_squared_error: 0.2690\n",
      "Epoch 24/100\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.3797 - mean_squared_error: 0.3797\n",
      "Epoch 25/100\n",
      "7/7 [==============================] - 0s 999us/step - loss: 0.3025 - mean_squared_error: 0.3025\n",
      "Epoch 26/100\n",
      "7/7 [==============================] - 0s 833us/step - loss: 0.2365 - mean_squared_error: 0.2365\n",
      "Epoch 27/100\n",
      "7/7 [==============================] - 0s 999us/step - loss: 0.2189 - mean_squared_error: 0.2189\n",
      "Epoch 28/100\n",
      "7/7 [==============================] - 0s 833us/step - loss: 0.2656 - mean_squared_error: 0.2656\n",
      "Epoch 29/100\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.2307 - mean_squared_error: 0.2307\n",
      "Epoch 30/100\n",
      "7/7 [==============================] - 0s 999us/step - loss: 0.2874 - mean_squared_error: 0.2874\n",
      "Epoch 31/100\n",
      "7/7 [==============================] - 0s 999us/step - loss: 0.3090 - mean_squared_error: 0.3090\n",
      "Epoch 32/100\n",
      "7/7 [==============================] - 0s 999us/step - loss: 0.2405 - mean_squared_error: 0.2405\n",
      "Epoch 33/100\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.2922 - mean_squared_error: 0.2922\n",
      "Epoch 34/100\n",
      "7/7 [==============================] - 0s 999us/step - loss: 0.2115 - mean_squared_error: 0.2115\n",
      "Epoch 35/100\n",
      "7/7 [==============================] - 0s 999us/step - loss: 0.2359 - mean_squared_error: 0.2359\n",
      "Epoch 36/100\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.2199 - mean_squared_error: 0.2199\n",
      "Epoch 37/100\n",
      "7/7 [==============================] - 0s 833us/step - loss: 0.2149 - mean_squared_error: 0.2149\n",
      "Epoch 38/100\n",
      "7/7 [==============================] - 0s 1000us/step - loss: 0.2436 - mean_squared_error: 0.2436\n",
      "Epoch 39/100\n",
      "7/7 [==============================] - 0s 999us/step - loss: 0.2791 - mean_squared_error: 0.2791\n",
      "Epoch 40/100\n",
      "7/7 [==============================] - 0s 833us/step - loss: 0.2252 - mean_squared_error: 0.2252\n",
      "Epoch 41/100\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.2575 - mean_squared_error: 0.2575\n",
      "Epoch 42/100\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.2319 - mean_squared_error: 0.2319\n",
      "Epoch 43/100\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.2476 - mean_squared_error: 0.2476\n",
      "Epoch 44/100\n",
      "7/7 [==============================] - 0s 833us/step - loss: 0.2450 - mean_squared_error: 0.2450\n",
      "Epoch 45/100\n",
      "7/7 [==============================] - 0s 833us/step - loss: 0.2851 - mean_squared_error: 0.2851\n",
      "Epoch 46/100\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.2507 - mean_squared_error: 0.2507\n",
      "Epoch 47/100\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.1957 - mean_squared_error: 0.1957\n",
      "Epoch 48/100\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.2910 - mean_squared_error: 0.2910\n",
      "Epoch 49/100\n",
      "7/7 [==============================] - 0s 833us/step - loss: 0.2727 - mean_squared_error: 0.2727\n",
      "Epoch 50/100\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.2720 - mean_squared_error: 0.2720\n",
      "Epoch 51/100\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.2565 - mean_squared_error: 0.2565\n",
      "Epoch 52/100\n",
      "7/7 [==============================] - 0s 833us/step - loss: 0.2238 - mean_squared_error: 0.2238\n",
      "Epoch 53/100\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.2093 - mean_squared_error: 0.2093\n",
      "Epoch 54/100\n",
      "7/7 [==============================] - 0s 999us/step - loss: 0.2276 - mean_squared_error: 0.2276\n",
      "Epoch 55/100\n",
      "7/7 [==============================] - 0s 999us/step - loss: 0.2001 - mean_squared_error: 0.2001\n",
      "Epoch 56/100\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.2053 - mean_squared_error: 0.2053\n",
      "Epoch 57/100\n",
      "7/7 [==============================] - 0s 833us/step - loss: 0.1959 - mean_squared_error: 0.1959\n",
      "Epoch 58/100\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.2346 - mean_squared_error: 0.2346\n",
      "Epoch 59/100\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.2040 - mean_squared_error: 0.2040\n",
      "Epoch 60/100\n",
      "7/7 [==============================] - 0s 999us/step - loss: 0.2363 - mean_squared_error: 0.2363\n",
      "Epoch 61/100\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.2180 - mean_squared_error: 0.2180\n",
      "Epoch 62/100\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.1918 - mean_squared_error: 0.1918\n",
      "Epoch 63/100\n",
      "7/7 [==============================] - 0s 833us/step - loss: 0.1797 - mean_squared_error: 0.1797\n",
      "Epoch 64/100\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.2354 - mean_squared_error: 0.2354\n",
      "Epoch 65/100\n",
      "7/7 [==============================] - 0s 999us/step - loss: 0.2190 - mean_squared_error: 0.2190\n",
      "Epoch 66/100\n",
      "7/7 [==============================] - 0s 999us/step - loss: 0.1758 - mean_squared_error: 0.1758\n",
      "Epoch 67/100\n",
      "7/7 [==============================] - 0s 999us/step - loss: 0.1887 - mean_squared_error: 0.1887\n",
      "Epoch 68/100\n",
      "7/7 [==============================] - 0s 999us/step - loss: 0.1927 - mean_squared_error: 0.1927\n",
      "Epoch 69/100\n",
      "7/7 [==============================] - 0s 999us/step - loss: 0.2148 - mean_squared_error: 0.2148\n",
      "Epoch 70/100\n",
      "7/7 [==============================] - 0s 833us/step - loss: 0.1722 - mean_squared_error: 0.1722\n",
      "Epoch 71/100\n",
      "7/7 [==============================] - 0s 833us/step - loss: 0.2081 - mean_squared_error: 0.2081\n",
      "Epoch 72/100\n",
      "7/7 [==============================] - 0s 833us/step - loss: 0.2170 - mean_squared_error: 0.2170\n",
      "Epoch 73/100\n",
      "7/7 [==============================] - 0s 999us/step - loss: 0.2092 - mean_squared_error: 0.2092\n",
      "Epoch 74/100\n",
      "7/7 [==============================] - 0s 833us/step - loss: 0.1674 - mean_squared_error: 0.1674\n",
      "Epoch 75/100\n",
      "7/7 [==============================] - 0s 999us/step - loss: 0.1865 - mean_squared_error: 0.1865\n",
      "Epoch 76/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 0s 833us/step - loss: 0.2158 - mean_squared_error: 0.2158\n",
      "Epoch 77/100\n",
      "7/7 [==============================] - 0s 833us/step - loss: 0.1727 - mean_squared_error: 0.1727\n",
      "Epoch 78/100\n",
      "7/7 [==============================] - 0s 833us/step - loss: 0.1972 - mean_squared_error: 0.1972\n",
      "Epoch 79/100\n",
      "7/7 [==============================] - 0s 999us/step - loss: 0.1843 - mean_squared_error: 0.1843\n",
      "Epoch 80/100\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.1911 - mean_squared_error: 0.1911\n",
      "Epoch 81/100\n",
      "7/7 [==============================] - 0s 833us/step - loss: 0.1950 - mean_squared_error: 0.1950\n",
      "Epoch 82/100\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.1851 - mean_squared_error: 0.1851\n",
      "Epoch 83/100\n",
      "7/7 [==============================] - 0s 832us/step - loss: 0.1590 - mean_squared_error: 0.1590\n",
      "Epoch 84/100\n",
      "7/7 [==============================] - 0s 999us/step - loss: 0.1645 - mean_squared_error: 0.1645\n",
      "Epoch 85/100\n",
      "7/7 [==============================] - 0s 1000us/step - loss: 0.1720 - mean_squared_error: 0.1720\n",
      "Epoch 86/100\n",
      "7/7 [==============================] - 0s 833us/step - loss: 0.1792 - mean_squared_error: 0.1792\n",
      "Epoch 87/100\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.1719 - mean_squared_error: 0.1719\n",
      "Epoch 88/100\n",
      "7/7 [==============================] - 0s 833us/step - loss: 0.1720 - mean_squared_error: 0.1720\n",
      "Epoch 89/100\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.2021 - mean_squared_error: 0.2021\n",
      "Epoch 90/100\n",
      "7/7 [==============================] - 0s 833us/step - loss: 0.1984 - mean_squared_error: 0.1984\n",
      "Epoch 91/100\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.1793 - mean_squared_error: 0.1793\n",
      "Epoch 92/100\n",
      "7/7 [==============================] - 0s 833us/step - loss: 0.1864 - mean_squared_error: 0.1864\n",
      "Epoch 93/100\n",
      "7/7 [==============================] - 0s 999us/step - loss: 0.2219 - mean_squared_error: 0.2219\n",
      "Epoch 94/100\n",
      "7/7 [==============================] - 0s 833us/step - loss: 0.1746 - mean_squared_error: 0.1746\n",
      "Epoch 95/100\n",
      "7/7 [==============================] - 0s 833us/step - loss: 0.1513 - mean_squared_error: 0.1513\n",
      "Epoch 96/100\n",
      "7/7 [==============================] - 0s 1000us/step - loss: 0.1842 - mean_squared_error: 0.1842\n",
      "Epoch 97/100\n",
      "7/7 [==============================] - 0s 833us/step - loss: 0.1987 - mean_squared_error: 0.1987\n",
      "Epoch 98/100\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.1803 - mean_squared_error: 0.1803\n",
      "Epoch 99/100\n",
      "7/7 [==============================] - 0s 833us/step - loss: 0.2208 - mean_squared_error: 0.2208\n",
      "Epoch 100/100\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.2134 - mean_squared_error: 0.2134\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000017E4F3AA700> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-0.5324425 ],\n",
       "       [ 0.6873611 ],\n",
       "       [-0.69899946],\n",
       "       [-0.8820946 ],\n",
       "       [ 1.1203946 ],\n",
       "       [ 0.6265653 ],\n",
       "       [-0.41871488],\n",
       "       [-0.7927043 ],\n",
       "       [ 0.897866  ],\n",
       "       [-0.9247256 ],\n",
       "       [ 0.26263925],\n",
       "       [ 0.4010286 ],\n",
       "       [ 1.1331146 ],\n",
       "       [-0.7034707 ],\n",
       "       [ 0.8557357 ],\n",
       "       [ 1.0308713 ],\n",
       "       [ 0.7497234 ],\n",
       "       [ 0.5962488 ],\n",
       "       [-0.8921069 ],\n",
       "       [-0.7563068 ],\n",
       "       [ 0.7992278 ],\n",
       "       [-0.81108606],\n",
       "       [-0.53309536],\n",
       "       [ 0.565876  ],\n",
       "       [ 0.51629436],\n",
       "       [ 0.8767411 ],\n",
       "       [-0.77432245],\n",
       "       [-0.8294679 ],\n",
       "       [ 0.55041444],\n",
       "       [-0.6867992 ],\n",
       "       [ 0.9194055 ]], dtype=float32)"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Feito a partir de: https://stackoverflow.com/questions/49008074/how-to-create-a-neural-network-for-regression\n",
    "# e a partir de: https://stackoverflow.com/questions/34716454/where-do-i-call-the-batchnormalization-function-in-keras\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.models import Sequential\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(data['x'].values.reshape((-1, 1)),data['y'], test_size=0.33, random_state=42)\n",
    "\n",
    "# Feature Scaling\n",
    "sc = StandardScaler()\n",
    "x_train = sc.fit_transform(x_train)\n",
    "x_test = sc.transform(x_test)\n",
    "\n",
    "# Initialising the ANN\n",
    "model = Sequential()\n",
    "\n",
    "# Adding the input layer and the first hidden layer\n",
    "model.add(Dense(128, activation = 'relu', input_dim=1))\n",
    "\n",
    "# Adding second layer\n",
    "model.add(Dense(64, activation = 'relu'))\n",
    "\n",
    "# Adding third layer\n",
    "model.add(Dense(32, activation = 'relu'))\n",
    "\n",
    "# Adding the output layer\n",
    "model.add(Dense(units = 1))\n",
    "\n",
    "# Compiling the ANN\n",
    "model.compile(optimizer = 'adam', loss = 'mean_squared_error', metrics=['mean_squared_error'])\n",
    "\n",
    "# Fitting the ANN to the Training set\n",
    "model.fit(x_train, y_train, epochs=100, batch_size=10)\n",
    "\n",
    "y_pred = model.predict(x_test)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "scientific-magazine",
   "metadata": {},
   "source": [
    "## Comparação\n",
    "\n",
    "Após 100 Epochs, o erro quadrático médio usando uma rede neural é bem menos do que na regressão linear (0.1803 < 0.507869)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
